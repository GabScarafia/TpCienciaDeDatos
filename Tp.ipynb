{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28b70cae",
   "metadata": {},
   "source": [
    "# Notebook de Recomendaciones de Peliculas\n",
    "La idea es que a partir de un prompt del usuario se recomienden peliculas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98481415",
   "metadata": {},
   "source": [
    "## Imports y using\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53aad557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\gscarafia\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from -r requirements.txt (line 1)) (2.3.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\gscarafia\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from -r requirements.txt (line 2)) (1.7.0)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\gscarafia\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from -r requirements.txt (line 3)) (4.1.0)\n",
      "Requirement already satisfied: faiss-cpu in c:\\users\\gscarafia\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from -r requirements.txt (line 4)) (1.11.0)\n",
      "Requirement already satisfied: wget in c:\\users\\gscarafia\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from -r requirements.txt (line 5)) (3.2)\n",
      "Requirement already satisfied: unzip in c:\\users\\gscarafia\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from -r requirements.txt (line 6)) (1.0.0)\n",
      "Collecting google-generativeai (from -r requirements.txt (line 8))\n",
      "  Using cached google_generativeai-0.8.5-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting spacy (from -r requirements.txt (line 9))\n",
      "  Using cached spacy-3.8.7-cp313-cp313-win_amd64.whl.metadata (28 kB)\n",
      "Collecting seaborn (from -r requirements.txt (line 10))\n",
      "  Using cached seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Ignored the following versions that require a different python version: 3.8.3 Requires-Python <3.13,>=3.9; 3.8.5 Requires-Python <3.13,>=3.9; 3.8.6 Requires-Python <3.13,>=3.9\n",
      "ERROR: Could not find a version that satisfies the requirement matplotlib.pyplot (from versions: none)\n",
      "ERROR: No matching distribution found for matplotlib.pyplot\n",
      "c:\\Users\\gscarafia\\AppData\\Local\\Programs\\Python\\Python313\\python.exe: No module named spacy\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m get_ipython().run_line_magic(\u001b[33m'\u001b[39m\u001b[33mpip\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33minstall -r requirements.txt\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      3\u001b[39m get_ipython().system(\u001b[33m'\u001b[39m\u001b[33mpython -m spacy download es_core_news_sm\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwordcloud\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m WordCloud\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt\n",
    "\n",
    "%python -m spacy download es_core_news_sm\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import os\n",
    "import wget\n",
    "import ast\n",
    "import google.generativeai as genai\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebe9378",
   "metadata": {},
   "source": [
    "## Generó Credenciales de Gemini "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d25c33f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'genai' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m GEMINI_API_KEY = \u001b[33m\"\u001b[39m\u001b[33mAIzaSyB2lqerBn4B8VuHHg53v7mZF3kdGmE-i7k\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Configurar la API de Google Gemini\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mgenai\u001b[49m.configure(api_key=GEMINI_API_KEY)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Listamos los modelos disponibles (lo usamos para debug)\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m genai.list_models():\n",
      "\u001b[31mNameError\u001b[39m: name 'genai' is not defined"
     ]
    }
   ],
   "source": [
    "# --- Configuración de la API de Gemini ---\n",
    "GEMINI_API_KEY = \"AIzaSyB2lqerBn4B8VuHHg53v7mZF3kdGmE-i7k\"\n",
    "\n",
    "# Configurar la API de Google Gemini\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "\n",
    "# Listamos los modelos disponibles (lo usamos para debug)\n",
    "for m in genai.list_models():\n",
    "    if \"generateContent\" in m.supported_generation_methods:\n",
    "        print(m.name)\n",
    "\n",
    "# Seleccionamos el modelo geminio que vamos a usar\n",
    "GEMINI_MODEL_NAME = \"gemini-2.0-flash\"\n",
    "\n",
    "try:\n",
    "    model = genai.GenerativeModel(GEMINI_MODEL_NAME)\n",
    "    print(f\"Modelo Gemini '{GEMINI_MODEL_NAME}' configurado exitosamente.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al configurar el modelo Gemini: {e}\")\n",
    "    print(\"Por favor, verifica tu clave API y tu conexión a internet.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e52a6c1",
   "metadata": {},
   "source": [
    "## Descarga de Dataset\n",
    "\n",
    "Se descarga de kaggle el movies-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08043ef2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m DATASET_CSV_NAME = \u001b[33m\"\u001b[39m\u001b[33mmovie_dataset.csv\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      5\u001b[39m DATASET_DIR = \u001b[33m\"\u001b[39m\u001b[33m./data/\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m DATASET_LOCAL_PATH = \u001b[43mos\u001b[49m.path.join(DATASET_DIR, DATASET_CSV_NAME)\n",
      "\u001b[31mNameError\u001b[39m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "# --- Configuración del dataset de películas ---\n",
    "DATASET_DOWNLOAD_URL = \"https://www.kaggle.com/api/v1/datasets/download/utkarshx27/movies-dataset\"\n",
    "DATASET_FILE_NAME = \"movies-dataset.zip\"\n",
    "DATASET_CSV_NAME = \"movie_dataset.csv\"\n",
    "DATASET_DIR = \"./data/\"\n",
    "DATASET_LOCAL_PATH = os.path.join(DATASET_DIR, DATASET_CSV_NAME)\n",
    "\n",
    "# Crea la carpeta de datos si no existe\n",
    "os.makedirs(DATASET_DIR, exist_ok=True)\n",
    "\n",
    "# Descarga el dataset si no existe localmente\n",
    "if not os.path.exists(DATASET_LOCAL_PATH):\n",
    "    print(f\"Descargando el dataset de películas (aprox. 23 MB) a: {DATASET_DIR}\")\n",
    "    print(\"Esto puede tardar unos segundos...\")\n",
    "    try:\n",
    "        # wget descarga el zip, luego lo descomprimimos\n",
    "        zip_path = os.path.join(DATASET_DIR, DATASET_FILE_NAME)\n",
    "        wget.download(DATASET_DOWNLOAD_URL, out=zip_path)\n",
    "        print(\"\\nDescarga del ZIP completada. Descomprimiendo...\")\n",
    "\n",
    "        import zipfile\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extract(DATASET_CSV_NAME, DATASET_DIR)\n",
    "        os.remove(zip_path) # Elimina el zip después de descomprimir\n",
    "        print(f\"Dataset '{DATASET_CSV_NAME}' descomprimido.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError al descargar o descomprimir el dataset: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d93481",
   "metadata": {},
   "source": [
    "## Preprocesamiento de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63294c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Preprocesamiento del dataset ---\n",
    "movies_df.rename(columns={'title': 'CleanTitle'}, inplace=True)\n",
    "\n",
    "movies_df.loc[:, 'Year'] = pd.to_datetime(movies_df['release_date'], errors='coerce').dt.year\n",
    "movies_df.loc[:, 'Year'] = movies_df['Year'].astype('Int64').fillna(0)\n",
    "movies_df.loc[movies_df['Year'] == 0, 'Year'] = 'año desconocido'\n",
    "\n",
    "# --- Funciones de parsing ---\n",
    "def parse_genres_robust(genres_str):\n",
    "    if pd.isna(genres_str) or genres_str == '[]' or genres_str == '':\n",
    "        return []\n",
    "    try:\n",
    "        genres_list = ast.literal_eval(genres_str)\n",
    "        if isinstance(genres_list, list):\n",
    "            return [d['name'] for d in genres_list if isinstance(d, dict) and 'name' in d]\n",
    "        else:\n",
    "            raise ValueError(\"Not a list of dictionaries\")\n",
    "    except (ValueError, SyntaxError):\n",
    "        if ',' in genres_str:\n",
    "            return [g.strip() for g in genres_str.split(',') if g.strip()]\n",
    "        else:\n",
    "            return [g.strip() for g in genres_str.split(' ') if g.strip()]\n",
    "    except Exception as e:\n",
    "        print(f\"Advertencia al parsear géneros: {e}\")\n",
    "        return []\n",
    "\n",
    "def parse_cast_robust(cast_str):\n",
    "    if pd.isna(cast_str) or cast_str == '[]' or cast_str == '':\n",
    "        return []\n",
    "    try:\n",
    "        cast_list = ast.literal_eval(cast_str)\n",
    "        if isinstance(cast_list, list):\n",
    "            return [d['name'] for d in cast_list if isinstance(d, dict) and 'name' in d]\n",
    "        else:\n",
    "            raise ValueError(\"Not a list of dictionaries\")\n",
    "    except (ValueError, SyntaxError):\n",
    "        if ',' in cast_str:\n",
    "            return [c.strip() for c in cast_str.split(',') if c.strip()]\n",
    "        else:\n",
    "            return [cast_str.strip()]\n",
    "\n",
    "# Aplica funciones de parsing\n",
    "movies_df['genres_parsed'] = movies_df['genres'].apply(parse_genres_robust)\n",
    "\n",
    "# ✅ Traducción de géneros del inglés al español\n",
    "genre_translation = {\n",
    "    \"Action\": \"Acción\",\n",
    "    \"Adventure\": \"Aventura\",\n",
    "    \"Comedy\": \"Comedia\",\n",
    "    \"Drama\": \"Drama\",\n",
    "    \"Science Fiction\": \"Ciencia Ficción\",\n",
    "    \"Science\": \"Ciencia\",\n",
    "    \"Fiction\": \"Ficción\",\n",
    "    \"Family\": \"Familiar\",\n",
    "    \"History\": \"Historia\",\n",
    "    \"Music\": \"Musical\",\n",
    "    \"Horror\": \"Terror\",\n",
    "    \"Thriller\": \"Thriller\",\n",
    "    \"Romance\": \"Romance\",\n",
    "    \"Animation\": \"Animación\",\n",
    "    \"Documentary\": \"Documental\",\n",
    "    \"Mystery\": \"Misterio\",\n",
    "    \"Fantasy\": \"Fantasía\",\n",
    "    \"Crime\": \"Crimen\",\n",
    "    \"War\": \"Guerra\",\n",
    "}\n",
    "\n",
    "def traducir_generos(generos):\n",
    "    return [genre_translation.get(g, g) for g in generos]\n",
    "\n",
    "movies_df['genres_traducidos'] = movies_df['genres_parsed'].apply(traducir_generos)\n",
    "movies_df['Genres'] = movies_df['genres_traducidos'].apply(lambda x: ', '.join(x) if x else '')\n",
    "\n",
    "# Cast\n",
    "movies_df['cast_parsed'] = movies_df['cast'].apply(parse_cast_robust)\n",
    "movies_df['CleanCast'] = movies_df['cast_parsed'].apply(lambda x: ', '.join(x) if x else '')\n",
    "\n",
    "# Promedio de rating\n",
    "movies_df.rename(columns={'vote_average': 'AvgRating'}, inplace=True)\n",
    "\n",
    "# Texto combinado para embeddings\n",
    "movies_df['combined_text'] = (\n",
    "    \"Título: \" + movies_df['CleanTitle'].fillna('') + \". \" +\n",
    "    \"Géneros: \" + movies_df['Genres'].fillna('') + \". \" +\n",
    "    \"Actores: \" + movies_df['CleanCast'].fillna('') + \". \" +\n",
    "    \"Sinopsis: \" + movies_df['overview'].fillna('')\n",
    ")\n",
    "\n",
    "# Elimina duplicados\n",
    "movies_df.drop_duplicates(subset=['CleanTitle', 'Year'], inplace=True, ignore_index=True)\n",
    "\n",
    "print(f\"Dataset preprocesado. Filas: {len(movies_df)}\")\n",
    "print(movies_df['combined_text'].sample(5).values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e8a5f6",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feac8c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos el modelo de embeddings\n",
    "#'paraphrase-multilingual-MiniLM-L12-v2' es poderoso para los multiples lenguajes\n",
    "print(\"Cargando modelo de embeddings (SentenceTransformer)...\")\n",
    "embedding_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "print(\"Modelo de embeddings cargado.\")\n",
    "\n",
    "# Genera embeddings para el dataset preprocesado\n",
    "print(\"Generando embeddings para las películas (esto puede tardar un poco)...\")\n",
    "movie_embeddings = embedding_model.encode(movies_df['combined_text'].tolist(), show_progress_bar=True)\n",
    "print(\"Embeddings generados.\")\n",
    "\n",
    "# Escala los embeddings\n",
    "scaler = StandardScaler()\n",
    "scaled_embeddings = scaler.fit_transform(movie_embeddings)\n",
    "\n",
    "# Crear el índice FAISS\n",
    "# D = dimensión de los embeddings (384 para all-MiniLM-L6-v2)\n",
    "D = scaled_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(D) # L2 para distancia euclidiana (común para embeddings)\n",
    "index.add(scaled_embeddings)\n",
    "print(f\"Índice FAISS creado con {index.ntotal} elementos.\")\n",
    "\n",
    "# Opcional: guardar/cargar el índice para no tener que regenerarlo\n",
    "# faiss.write_index(index, \"movie_embeddings.faiss\")\n",
    "# index = faiss.read_index(\"movie_embeddings.faiss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5c5389",
   "metadata": {},
   "source": [
    "## Nube de palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442c64f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "# Extraer todos los géneros individuales desde la columna 'genres_traducidos'\n",
    "todos_los_generos = [genero for sublist in movies_df['genres_traducidos'] for genero in sublist]\n",
    "\n",
    "# Contar frecuencias\n",
    "frecuencia_generos = Counter(todos_los_generos)\n",
    "\n",
    "# Crear la nube de palabras\n",
    "wordcloud = WordCloud(\n",
    "    width=800,\n",
    "    height=400,\n",
    "    background_color='white',\n",
    "    colormap='tab10'\n",
    ").generate_from_frequencies(frecuencia_generos)\n",
    "\n",
    "# Mostrarla\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Frecuencia de Géneros')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50285cf",
   "metadata": {},
   "source": [
    "## Procesamiento de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275489af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Cargar el modelo de spaCy para español\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# Lista de géneros en español para detectar desde el prompt\n",
    "GENRES = [\n",
    "    \"acción\", \"aventura\", \"comedia\", \"drama\", \"ciencia ficción\", \"terror\", \"thriller\", \n",
    "    \"romance\", \"animación\", \"documental\", \"misterio\", \"fantasía\", \"crimen\"\n",
    "]\n",
    "\n",
    "# Extrae nombres de actores y géneros del prompt\n",
    "def extract_actor_and_genres(prompt):\n",
    "    doc = nlp(prompt)\n",
    "    actores = [ent.text for ent in doc.ents if ent.label_ == \"PER\"]  # spaCy reconoce nombres de personas\n",
    "\n",
    "    generos_encontrados = []\n",
    "    prompt_lower = prompt.lower()\n",
    "    for genero in GENRES:\n",
    "        if genero in prompt_lower:\n",
    "            generos_encontrados.append(genero.capitalize())  # Para coincidir con columna Genres\n",
    "\n",
    "    return actores, generos_encontrados\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "def recommend_movies_mejorado(prompt, k=5):\n",
    "    actores, generos = extract_actor_and_genres(prompt)\n",
    "    df_filtrado = movies_df.copy()\n",
    "\n",
    "    if actores:\n",
    "        df_filtrado = df_filtrado[df_filtrado['CleanCast'].str.contains('|'.join(actores), case=False, na=False)]\n",
    "    if generos:\n",
    "        df_filtrado = df_filtrado[df_filtrado['Genres'].str.contains('|'.join(generos), case=False, na=False)]\n",
    "\n",
    "    if df_filtrado.empty:\n",
    "        df_filtrado = movies_df.copy()\n",
    "\n",
    "    # Filtrar los embeddings precalculados\n",
    "    indices_filtrados = df_filtrado.index.tolist()\n",
    "    filtered_embeddings = scaled_embeddings[indices_filtrados]\n",
    "\n",
    "    # Crear índice FAISS temporal con embeddings filtrados\n",
    "    D = filtered_embeddings.shape[1]\n",
    "    temp_index = faiss.IndexFlatL2(D)\n",
    "    temp_index.add(filtered_embeddings)\n",
    "\n",
    "    # Embedding del prompt\n",
    "    prompt_emb = embedding_model.encode([prompt])\n",
    "    scaled_prompt_emb = scaler.transform(prompt_emb)\n",
    "\n",
    "    distances, indices = temp_index.search(scaled_prompt_emb, k)\n",
    "    recomendadas = df_filtrado.iloc[indices[0]]\n",
    "\n",
    "    return recomendadas.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9731a9a",
   "metadata": {},
   "source": [
    "## llamado a Gemini "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cac1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llamar_a_gemini(prompt_texto):\n",
    "    \"\"\"\n",
    "    Llama a la API de Google Gemini para generar una respuesta.\n",
    "    \"\"\"\n",
    "    model = genai.GenerativeModel(GEMINI_MODEL_NAME)\n",
    "\n",
    "\n",
    "    try:\n",
    "        response = model.generate_content(\n",
    "            prompt_texto,\n",
    "            generation_config=genai.types.GenerationConfig(\n",
    "                temperature=0.7,\n",
    "                max_output_tokens=500,\n",
    "            )\n",
    "        )\n",
    "        # El texto generado está en response.candidates[0].content.parts[0].text\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        print(f\"Error al llamar a la API de Gemini: {e}\")\n",
    "        return \"Lo siento, no pude generar una recomendación en este momento. Por favor, intenta de nuevo más tarde.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f8c0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = input(\"¿Qué tipo de película querés ver?: \")\n",
    "\n",
    "recommendations = recommend_movies_mejorado(user_prompt, k=5)\n",
    "\n",
    "resumen = \"\"\n",
    "for i, row in recommendations.iterrows():\n",
    "    movie_year = int(row['Year']) if pd.notna(row['Year']) and row['Year'] != 'año desconocido' else 'año desconocido'\n",
    "\n",
    "    resumen += (\n",
    "        f\"Título: {row['CleanTitle']} ({movie_year}), \"\n",
    "        f\"Géneros: {row['Genres']}, \"\n",
    "        f\"Actores: {row['CleanCast']}, \"\n",
    "        f\"Rating Promedio: {round(row['AvgRating'], 1)}.\\n\"\n",
    "        f\"Sinopsis: {row['overview']}\\n\\n\"\n",
    "    )\n",
    "prompt_llm = f\"\"\"\n",
    "Actuá como un recomendador de películas en español. Un usuario te dijo lo siguiente:\n",
    "\"{user_prompt}\"\n",
    "\n",
    "Estas son tus opciones (con título, año, géneros, actores, rating promedio y sinopsis):\n",
    "{resumen}\n",
    "\n",
    "Considerando la sinopsis, los géneros, los actores y el rating de las películas, respondé en tono natural y conversacional, recomendando las películas que mejor se ajusten a la preferencia del usuario.\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nGenerando recomendación con Google Gemini (API)...\")\n",
    "respuesta = llamar_a_gemini(prompt_llm)\n",
    "print(\"\\n🎬 Recomendación personalizada:\\n\")\n",
    "print(respuesta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c66d0e7",
   "metadata": {},
   "source": [
    "## Visualizacion de Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cb6580",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Análisis de las características de las películas recomendadas ---\")\n",
    "\n",
    "# Distribución de Géneros en las Recomendaciones\n",
    "print(\"\\nGéneros más comunes en las recomendaciones:\")\n",
    "# Aplanar la lista de géneros y contar ocurrencias\n",
    "all_genres = []\n",
    "for genres_str in recommendations['Genres']:\n",
    "    all_genres.extend([g.strip() for g in genres_str.split(',') if g.strip()])\n",
    "genre_counts = pd.Series(all_genres).value_counts().head(10)\n",
    "print(genre_counts)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=genre_counts.values, y=genre_counts.index, palette=\"viridis\")\n",
    "plt.title('Top 10 Géneros en Películas Recomendadas')\n",
    "plt.xlabel('Número de Películas')\n",
    "plt.ylabel('Género')\n",
    "plt.show()\n",
    "\n",
    "# Distribución de Ratings Promedio en las Recomendaciones\n",
    "print(\"\\nDistribución de Ratings Promedio en las recomendaciones:\")\n",
    "print(recommendations['AvgRating'].describe())\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(recommendations['AvgRating'], bins=5, kde=True)\n",
    "plt.title('Distribución de Ratings Promedio de Películas Recomendadas')\n",
    "plt.xlabel('Rating Promedio')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.show()\n",
    "\n",
    "# Distribución de Años en las Recomendaciones\n",
    "print(\"\\nAños de lanzamiento de las películas recomendadas:\")\n",
    "# Convertir 'año desconocido' a NaN para la graficación numérica, luego manejarlo por separado si es necesario\n",
    "years_for_plot = recommendations['Year'].apply(lambda x: np.nan if x == 'año desconocido' else int(x))\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(years_for_plot.dropna(), bins=5, kde=False)\n",
    "plt.title('Distribución de Años de Películas Recomendadas')\n",
    "plt.xlabel('Año de Lanzamiento')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.show()\n",
    "if 'año desconocido' in recommendations['Year'].values:\n",
    "    print(f\"Hay {sum(recommendations['Year'] == 'año desconocido')} película(s) con año desconocido en las recomendaciones.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
